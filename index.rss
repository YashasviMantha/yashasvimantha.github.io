<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
  xmlns:content="http://purl.org/rss/1.0/modules/content/"
  xmlns:dc="http://purl.org/dc/elements/1.1/"
  xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd"
  xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
  <channel>
    <title>YM RSS Feed</title>
    <link>https://yashasvimantha.com</link>
    <description>The official RSS Feed for https://yashasvimantha.com</description>
    <pubDate>Sat, 28 Jun 2025 09:05:03 +0000</pubDate>
    <item>
      <title>Matryoshka Embeddings - Finding the sweet spot between Embedding size and retrieval quality for Qwen3 0.6b</title>
      <link>https://yashasvimantha.com/posts/matryoshka-tradeoff/</link>
      <description>&lt;h1 id=&quot;matryoshka-embeddings---finding-the-sweet-spot-between-embedding-size-and-retrieval-quality-for-qwen3-06b&quot;&gt;Matryoshka Embeddings - Finding the sweet spot between Embedding size and retrieval quality for Qwen3 0.6b&lt;/h1&gt;

&lt;p&gt;2025-06-28&lt;/p&gt;

&lt;p&gt;We all know about Matryoshka embeddings. If not, its basically the way an embeddings model is trained where we can truncate the embeddings to a given size and still have not so much of a quality drop. There are a lot of advantages of truncating the embeddings to a lower size. One of them being; its cheaper to store the vectors. This becomes important as we scale things up for search. When we have a model that gives out a vector of size &lt;code&gt;1024&lt;/code&gt; and have to store millions of documents that cost of storing those can add up quite considerably. Especially when we have to store the vectors in memory for low latency. So Matryoshka Representation learning can help us bring down storage costs along with having a minimal quality drop.&lt;/p&gt;

&lt;p&gt;But the question, how much smaller can we go? Obviously just like most things in life, there is a trade-off between quality and compute/storage. This blog is about answering that.&lt;/p&gt;

&lt;p&gt;Choosing a model (that provides a outputs size of 1024 for example); how small can I slice to optimize trade-off?&lt;/p&gt;

&lt;p&gt;Heavily inspired from the &lt;a href=&quot;https://docs.vespa.ai/en/tutorials/hybrid-search.html&quot;&gt;Hybrid Search&lt;/a&gt; post from the vespa team. I thought, lets take a small dataset, brute force the evaluation with different embeddings size and find out.&lt;/p&gt;

&lt;p&gt;So I did exactly that in &lt;a href=&quot;https://github.com/YashasviMantha/matryoshka-analysis/blob/main/analysis.ipynb&quot;&gt;this notebook&lt;/a&gt;. And I wanted to test out the most recent new &lt;a href=&quot;https://huggingface.co/Qwen/Qwen3-Embedding-0.6B&quot;&gt;Qwen3 Embeddings 0.6B&lt;/a&gt; model.&lt;/p&gt;

&lt;p&gt;The testing set I used was the &lt;code&gt;BEIR&lt;/code&gt; slice of &lt;code&gt;nfcorpus&lt;/code&gt;. I calculated the embeddings at full capacity of 1024. And tested right from &lt;code&gt;1&lt;/code&gt; to &lt;code&gt;1024&lt;/code&gt; embeddings size on the test set; with my evaluation method to be &lt;code&gt;nDCG@10&lt;/code&gt;. While I do realize the model isn’t designed to go smaller than &lt;code&gt;32&lt;/code&gt;; I was curious on the performance in smaller sizes.&lt;/p&gt;

&lt;p&gt;The results were quite surprising.&lt;/p&gt;

&lt;p&gt;Some things to keep in mind before getting to the results:
- The test set is pretty small but a well respected benchmark.
- &lt;code&gt;nDCG@10&lt;/code&gt; may change with different HSNW configurations. I used chromaDB for the graph
- Source code present in the &lt;a href=&quot;https://github.com/YashasviMantha/matryoshka-analysis&quot;&gt;github repo&lt;/a&gt;.
- Ranking is a very data specific task. Similar results might not be reproducible with a different dataset.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results:&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/public/images/Matryoshka%20Embeddings%20Graph%20of%20performance%20and%20embeddings%20size.png&quot; alt=&quot;Graph of performance and embeddings size&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;bigger-is-not-always-better&quot;&gt;Bigger is not always better:&lt;/h3&gt;
&lt;p&gt;Obviously, if we dont slice the embeddings at all; we see the higest score. But around the &lt;code&gt;100-120&lt;/code&gt; range the performance actually goes down. So if someone was considering a 120 size embeddings, it would be better to go lower to 100 than 120.&lt;/p&gt;

&lt;p&gt;Similarly the quality doesnt always increase with dimensions. &lt;code&gt;762-766&lt;/code&gt; provides better retrieval than &lt;code&gt;768-774&lt;/code&gt;. The increase is always not linear. Or maybe I am being a bit too picky?&lt;/p&gt;

&lt;h3 id=&quot;the-sweet-spot&quot;&gt;The sweet spot&lt;/h3&gt;
&lt;p&gt;Now, it depends on the retrieval use case but for me, its seems like is around the early &lt;code&gt;400&lt;/code&gt; range is a good trade off. I say this because the incremental gain above &lt;code&gt;450&lt;/code&gt; might be too costly in a resource constrained environment.&lt;/p&gt;

&lt;p&gt;Also above the &lt;code&gt;650&lt;/code&gt; the gain becomes very small. Almost next to nothing.&lt;/p&gt;

&lt;h3 id=&quot;drilling-down&quot;&gt;Drilling Down&lt;/h3&gt;
&lt;p&gt;If we wanted to go crazy and also check how its behaving above &lt;code&gt;650&lt;/code&gt; in a more granular way:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/images/Matryoshka%20Embeddings%20Graph%20of%20performance%20and%20embeddings%20size%20650%20Slice.png&quot; alt=&quot;Graph of performance and embeddings size&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again, we are looking into numbers that are very close to each other.&lt;/p&gt;

&lt;h2 id=&quot;finally&quot;&gt;Finally&lt;/h2&gt;
&lt;p&gt;The goal here was not to find the perfect embedding size; but to understand if the quality reduction was considerable on size reduction. We can almost half the embedding size and not make the retrieval quality really bad. Which is huge because cutting memory into half is worth exploring. We need a lot more scrutiny here (larger datasets, multiple ranking metrics etc). But I will be using a 512 (for some reason I like this number) on my retrieval tasks.&lt;/p&gt;

&lt;p&gt;If given the choice of either use a single 1024 model or 2 models trained differently but support MLR; I will always pick the latter.&lt;/p&gt;

&lt;p&gt;Let me know if I missed/got something wrong here.&lt;/p&gt;
</description>
      <pubDate>Sat, 28 Jun 2025 12:00:00 +0000</pubDate>
      <dc:date>2025-06-28T12:00:00+00:00</dc:date>
    </item>
    <item>
      <title>Minimal Design: Why I shifted to my current website template (wruby).</title>
      <link>https://yashasvimantha.com/posts/minimal-design/</link>
      <description>&lt;h1 id=&quot;minimal-design-why-i-shifted-to-my-current-website-template-wruby&quot;&gt;Minimal Design: Why I shifted to my current website template (wruby).&lt;/h1&gt;

&lt;p&gt;2025-03-20&lt;/p&gt;

&lt;p&gt;I have come to realizing that all the websites on the internet (atleast a major chunk of them) as being over engineered.&lt;/p&gt;

&lt;p&gt;For example my old website was extremely bloated, over engineered, sloggy and slow to load. It fetched a shit ton of &lt;code&gt;js&lt;/code&gt; and &lt;code&gt;css&lt;/code&gt; scripts and it was pretty too complex to make any small change. It was so bloated that I went on a spree to remove random chunks of source code and the website seemed to work the same. Maybe some of chunks were not directly but indirectly necessary. Nevertheless, the point I am trying to make is; it had a lot of ‘not so useful’ content.&lt;/p&gt;

&lt;p&gt;But it did have really good, sophisticated UI with all kinds of animations, parallax effects, beautiful colors etc etc. Infact, it was a bit too fancy interms of design. And somehow didnt represent me a lot of ways. Which is also why I hesitated with sharing it to the public.&lt;/p&gt;

&lt;p&gt;Also one of the reason I think it was bloated, was, it having SEO optimization, page optimizations, caching on a CDN or &lt;code&gt;server-side&lt;/code&gt; rendering etc. Which is probably needed for a large scale webapp. But its definetly not needed for a personal blog/website.&lt;/p&gt;

&lt;p&gt;A good example that comes to mind in context of this is a comment made by a friend, a developer at a large scale platform (I relate with the message in some way), but apparently, there was a situation where a bug on the authentication layer on the platform was reported. But for different reasons this bug was not prioritized. Not sure why but maybe it was not a bug worth fixing. Or maybe it was too much of an effort to fix. But obviously, its a bug none the less. Which can potentially lead to an authentication loophole. The engineers replied, the worst thing that can happend on this is that the exploiter can access very low impact parts of the platform. And he will have to one of the best security engineers to do that.&lt;/p&gt;

&lt;p&gt;Obviously not a reply that you wanna hear at a board meeting or anything. But I want to focous on the spirit of the reply and not the reply itself: Not everything needs the state-of-the-art systems. Systems have to be, in the long term: easy to maintain, bloat free and most importantly, fast. They have to be extreamly fast. For example this website on its bare bones is less than a single MB (excluding all the static PDFs that are commited). Which by definition makes it lightning fast thanks to its small size. I just dont understand why we are okay with things being slow as long as they look fancy. At this point we are used to the buffering circle spinning for 4-5 seconds and have stopped realising, “why does it take so long to load..!” Ofcourse, this excludes large data stuff like videos and images.&lt;/p&gt;

&lt;p&gt;Among a lot of reasons like bloated code base; unoptimized api calling etc, One of the major reasons is tracking. Literally everything is on tracking these days with URL shortning and analytics. And I mean everything as simple as button clicks and scrolling hooks. Maybe this is why people really like retro tech. They are durable, fast and maintenance free (okay maybe not entirely) and most importantly, tracking free. A good example on this was &lt;a href=&quot;https://www.mcmaster.com/&quot;&gt;McMaster-Carr’s&lt;/a&gt; website. It looks a 100 years old and they have an extreamly fast website.&lt;/p&gt;

&lt;p&gt;A similar thing is happening with AI as well now. Everything from cars to cycles are becoming “smart”. Now dont get me wrong, a lot of things can provide a lot of value being smart. But not everything. I recently saw an AD on a newspaper where a rice cooker had some “AI” features in them. ITS RICE!! We dont need AI to cook rice. We need it to solve problems that were difficult to solve otherwise like driving or information retrieval. When did we run out of AI-worthy problems that now AI is being unnecessarily shoved down into ‘If else ladder’ solutions.&lt;/p&gt;

&lt;p&gt;Another school of thought that I have come across while trying to debloat my webiste and make it simple was a philosophy “you need complex systems to make solutions simple” (Again looking at &lt;a href=&quot;https://www.mcmaster.com/&quot;&gt;McMaster-Carr’s&lt;/a&gt;). Its definetly an interesting one. But I would prefer everything minimal. Even if it means sometimes trading off some features.&lt;/p&gt;

&lt;p&gt;My website currently uses a minimal site generator, &lt;a href=&quot;https://wruby.btxx.org/&quot;&gt;wruby&lt;/a&gt;. But its not off the shelf. I customized it to my liking. The best part of wruby is; its extreamly fast. Mostly because its bare bones and just html css. No jquery almost no javascript. The builder itself is written in ruby and is a single script that converts MD files to HTML and also replaces placeholders. Pushing for a good balance with simplicity and functionality. It also generates a RSS file (a big win in my book). So you can get RSS updates on all my blog posts which I plan on posting here along with copies on other websites. All this was to just make my website minimal and fast. It has no tracking or analytics; at least for now (until I get a very strong reason to start observability). Just simple plain HTML and CSS files converted from markdown files.&lt;/p&gt;

&lt;p&gt;I came across wruby when I was scrolling throught this community called the &lt;a href=&quot;https://1mb.club/&quot;&gt;1MB club&lt;/a&gt;. This community has a collection of websites which are performance focused. And there are a lot. Going through them fealt like a fresh breeze of air. There are collections of websites of all kinds including some of them being company homepages, personal websites, retro websites etc etc.&lt;/p&gt;

&lt;p&gt;But anyway, I think I have finally choosen my long term website building template/framework. And combined with github actions, I basically have a fully automated CICD workflow that can reduce my TAT (turn around time) from push to release in under a minute. Extremely cool, simple, fast and minimal. And all in my control including the builder.&lt;/p&gt;

&lt;p&gt;So I have finally officially launched my website (in other words, will start posting the URL on public platforms) after owning my current domain name for over 5 years. While I had my previous webiste I never got the urge to “launch” it. I guess it just didnt click with me. In case you were curious just add a &lt;code&gt;dev&lt;/code&gt; infront of the domain name to check out the old website template (it was from html5up.net, bug shoutout to them). I deployed it when I was in my 3rd year uni. I spent a lot more time customizing the old one than I did on this one.&lt;/p&gt;

&lt;p&gt;TLDR: Just stop over engineering shit when its not necessary.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;br /&gt;
YM&lt;/p&gt;
</description>
      <pubDate>Thu, 20 Mar 2025 12:00:00 +0000</pubDate>
      <dc:date>2025-03-20T12:00:00+00:00</dc:date>
    </item>
    <dc:date>2025-06-28T09:05:03+00:00</dc:date>
  </channel>
</rss>